# PCA MLE (Maximum Likelihood Estimation) Analysis Report

## Overview

This document provides a comprehensive analysis of the `pca_mle.py` implementation for dimensionality reduction using Maximum Likelihood Estimation approach.

**Dataset:** Amazon Movies & TV Reviews
- **Items:** 11,123
- **Ratings:** 2,149,655

**Target Users:** U1=134471, U2=27768, U3=16157
**Target Items:** I1=1333 (mean=3.69), I2=1162 (mean=3.74)

---

## Implementation Phases

### Phase 0: Data Loading & Item Mean Calculation (Preparation)
- Loads item average ratings and target users/items
- Creates item means dictionary

---

### Phase 1 (Point 1): Generate MLE Covariance Matrix

**Key Difference from Mean-Filling:**
- Divides by `|Common(i,j)| - 1` (number of users who rated BOTH items minus 1)
- If `|Common(i,j)| < 2`, sets `Cov(i,j) = 0`

**Formula:**
```
Cov_MLE(i,j) = Σ_{u ∈ Common(i,j)} (R_{u,i} - μᵢ)(R_{u,j} - μⱼ) / (|Common(i,j)| - 1)

where Common(i,j) = {users who rated BOTH items i and j}
```

**Output:** 11,123 × 11,123 covariance matrix

**Covariance Values for Target Items:**
| Metric | Value |
|--------|-------|
| Var(I1) | 0.557218 |
| Var(I2) | 0.636872 |
| Cov(I1, I2) | 0.134281 |

---

### Phase 2 (Point 2): Eigen Decomposition + Top Peers

Computes eigenvalues and eigenvectors of the MLE covariance matrix.

**Top 10 Eigenvalues:**
| PC | Eigenvalue |
|----|------------|
| λ₁ | 430.304627 |
| λ₂ | 189.063894 |
| λ₃ | 163.208304 |
| λ₄ | 159.948145 |
| λ₅ | 154.299303 |
| λ₆ | 136.400185 |
| λ₇ | 133.385840 |
| λ₈ | 125.410466 |
| λ₉ | 123.279958 |
| λ₁₀ | 117.286493 |

**Total Variance:** 7926.099340

**Variance Explained:**
| PCs | Variance Explained |
|-----|-------------------|
| Top-5 | 13.84% |
| Top-10 | 21.86% |

**Projection Matrices:**
- W_top5: (11,123 × 5)
- W_top10: (11,123 × 10)

**Top Peers:** Determines top 5 and top 10 peers for I1 and I2

**Formula:**
```
Σ_MLE × W = W × Λ  (eigenvalue equation)

Variance Explained = Σᵢ₌₁ᵏ λᵢ / Σᵢ₌₁ⁿ λᵢ × 100%

Reconstructed Σ ≈ W × Λ × Wᵀ
```

---

### Phase 3 (Point 3 & 5): Reduced Dimensional Space

Projects users into reduced latent space:

**Formula:** `t_{u,p} = Σ_{j in Observed(u)} (R_{u,j} - μ_j) × W_{j,p}`

- **Point 3:** Projects users to 5D space (Top-5 PCs)
- **Point 5:** Projects users to 10D space (Top-10 PCs)

**Formula:**
```
t_{u,p} = Σ_{j ∈ Observed(u)} (R_{u,j} - μⱼ) × W_{j,p}

UserScore_u = [t_{u,1}, t_{u,2}, ..., t_{u,k}]
```

---

### Phase 4 (Point 4 & 6): Rating Predictions

**Formula:** `r_hat_{u,i} = μ_i + Σ_{p=1}^k (t_{u,p} × W_{i,p})`

- **Point 4:** Predictions using Top-5 PCs
- **Point 6:** Predictions using Top-10 PCs

**Formula (Reconstruction):**
```
r̂_{u,i} = μᵢ + Σ_{p=1}^k (t_{u,p} × W_{i,p})

where:
- μᵢ = mean rating of item i
- t_{u,p} = user u's score on principal component p
- W_{i,p} = loading of item i on principal component p
```

---


## Terminal Results

### Prediction Results (Top-5 PCs)

| User | Item | Predicted | Actual (Item Mean) | Error |
|------|------|-----------|-------------------|-------|
| U1 | I1 | 3.69 | 3.69 | 0.00 |
| U1 | I2 | 3.74 | 3.74 | 0.00 |
| U2 | I1 | 3.65 | 3.69 | 0.04 |
| U2 | I2 | 3.69 | 3.74 | 0.05 |
| U3 | I1 | 3.65 | 3.69 | 0.04 |
| U3 | I2 | 3.67 | 3.74 | 0.07 |

**Top-5 PCs Average Error:** 0.033

### Prediction Results (Top-10 PCs)

| User | Item | Predicted | Actual (Item Mean) | Error |
|------|------|-----------|-------------------|-------|
| U1 | I1 | 3.69 | 3.69 | 0.00 |
| U1 | I2 | 3.74 | 3.74 | 0.00 |
| U2 | I1 | 3.66 | 3.69 | 0.03 |
| U2 | I2 | 3.71 | 3.74 | 0.03 |
| U3 | I1 | 3.70 | 3.69 | 0.00 |
| U3 | I2 | 3.75 | 3.74 | 0.02 |

**Top-10 PCs Average Error:** 0.013

---

## Top-5 PCs vs Top-10 PCs Comparison

| Metric | Top-5 PCs | Top-10 PCs |
|--------|-----------|------------|
| **Avg Error** | 0.033 | **0.013** |
| **Min Error** | 0.00 | 0.00 |
| **Max Error** | 0.07 | 0.03 |
| **Zero Errors** | 2 | 3 |

### Key Observations:

1. **Top-10 PCs performs better** with 60% lower average error (0.013 vs 0.033)

2. **U1 (Low Activity User)** has 0.0 error in both cases, predictions equal to item mean

3. **More PCs = Better Reconstruction** - captures more variance for accurate predictions

4. **MLE predictions cluster around item mean** - reasonable for sparse users

---

## MLE Method Characteristics

### Advantages:
1. Uses only observed data (no artificial mean-filling)
2. More statistically rigorous for sparse matrices
3. Predictions naturally regress toward item mean for sparse users

### Limitations:
1. Computationally expensive (pairwise user comparisons)
2. May underestimate covariance for items with few common raters
3. Zero covariance for item pairs with < 2 common users

---

## Output Files Generated

1. `mle_covariance_matrix.csv` - Full MLE covariance matrix
2. `mle_target_item_peers.csv` - Top 5 & 10 peers for I1 and I2
3. `mle_predictions.csv` - All prediction results
4. `mle_top5_eigenvalues.csv` - Top 5 eigenvalues
5. `mle_top10_eigenvalues.csv` - Top 10 eigenvalues
6. `mle_covariance_before_reduction.csv` - Original covariance for targets
7. `mle_covariance_after_top5.csv` - Reconstructed covariance (Top-5)
8. `mle_covariance_after_top10.csv` - Reconstructed covariance (Top-10)

---

## Comparison: Reduced Dimensional Space (Point 3 vs Point 6)

### Point 3: Top-5 PCs (5D Space)
### Point 6: Top-10 PCs (10D Space)

**Sample User Scores (from terminal):**

| User | Top-5 Scores (5D) | Top-10 Scores (10D) |
|------|-------------------|---------------------|
| U3 | [4.95, -1.27, 2.95, 0.13, -0.48] | [4.95, -1.27, 2.95, 0.13, -0.48, -6.46, 5.56, 5.35, -8.80, -8.86] |

### Prediction Comparison

| User | Item | Top-5 (Point 3) | Top-10 (Point 6) | Better |
|------|------|-----------------|------------------|--------|
| U1 | I1 | 3.69 (err=0.00) | 3.69 (err=0.00) | Tie |
| U1 | I2 | 3.74 (err=0.00) | 3.74 (err=0.00) | Tie |
| U2 | I1 | 3.65 (err=0.04) | 3.66 (err=0.03) | **Top-10** |
| U2 | I2 | 3.69 (err=0.05) | 3.71 (err=0.03) | **Top-10** |
| U3 | I1 | 3.65 (err=0.04) | 3.70 (err=0.00) | **Top-10** |
| U3 | I2 | 3.67 (err=0.07) | 3.75 (err=0.02) | **Top-10** |

### Summary Statistics

| Metric | Top-5 PCs (Point 3) | Top-10 PCs (Point 6) |
|--------|---------------------|----------------------|
| Total Error | 0.20 | 0.08 |
| Avg Error | 0.033 | **0.013** |
| Improvement | - | **60% better** |

### Comments

1. **Top-10 PCs wins in 4 out of 6 cases** - The additional 5 dimensions capture more user preference patterns

2. **U1 has identical predictions** - Low activity users converge to item mean regardless of dimensionality

3. **U3 shows biggest improvement** - From 0.04/0.07 errors to 0.00/0.02, showing that active users benefit more from additional dimensions

4. **Trade-off consideration:**
   - Top-5: Less computation, good for sparse users
   - Top-10: Better accuracy, recommended for active users

**Recommendation:** Use Top-10 PCs for better prediction accuracy when computational resources allow.

---

## Comparison: Mean-Fill (Point 9) vs MLE (Point 4) - Top-5 PCs

### Methods Compared:
- **Mean-Fill Point 9:** k-NN with cosine similarity in 5D latent space
- **MLE Point 4:** Reconstruction formula (μ_i + Σ t_u,p × W_i,p) in 5D latent space

### Prediction Results (Top-5 PCs)

| User | Item | Mean-Fill (Point 9) | MLE (Point 4) | Better |
|------|------|---------------------|---------------|--------|
| U1 | I1 | 3.30 (err=0.39) | 3.69 (err=0.00) | **MLE** |
| U1 | I2 | 3.10 (err=0.64) | 3.74 (err=0.00) | **MLE** |
| U2 | I1 | 3.20 (err=0.49) | 3.65 (err=0.04) | **MLE** |
| U2 | I2 | 3.35 (err=0.39) | 3.69 (err=0.05) | **MLE** |
| U3 | I1 | 2.85 (err=0.84) | 3.65 (err=0.04) | **MLE** |
| U3 | I2 | 3.45 (err=0.29) | 3.67 (err=0.07) | **MLE** |

### Summary Statistics

| Metric | Mean-Fill (Point 9) | MLE (Point 4) |
|--------|---------------------|---------------|
| Total Error | 3.04 | 0.20 |
| Avg Error | 0.507 | **0.033** |
| Min Error | 0.29 | 0.00 |
| Max Error | 0.84 | 0.07 |
| Improvement | - | **93% better** |

### Comments

1. **MLE dramatically outperforms Mean-Fill** - 93% lower average error (0.033 vs 0.507)

2. **Root Cause of Difference:**
   - Mean-Fill uses k-NN which finds similar users, but their ratings may be biased lower
   - MLE reconstruction naturally centers predictions around item mean

3. **Mean-Fill has downward bias** - All predictions are below item mean (2.85-3.45), indicating neighbors tend to rate lower

4. **MLE stability** - Predictions stay close to item mean (3.65-3.74), more stable for sparse users

5. **Prediction Formula Impact:**
   - Mean-Fill: μ_i + Σ(sim × centered_rating) / Σ|sim| → vulnerable to neighbor selection
   - MLE: μ_i + Σ(t_u,p × W_i,p) → direct reconstruction, more robust

**Recommendation:** MLE method is significantly better for rating prediction, especially for users without many observed ratings on target items.

---

## Comparison: Mean-Fill (Point 11) vs MLE (Point 6) - Top-10 PCs

### Methods Compared:
- **Mean-Fill Point 11:** k-NN with cosine similarity in 10D latent space
- **MLE Point 6:** Reconstruction formula (μ_i + Σ t_u,p × W_i,p) in 10D latent space

### Prediction Results (Top-10 PCs)

| User | Item | Mean-Fill (Point 11) | MLE (Point 6) | Better |
|------|------|----------------------|---------------|--------|
| U1 | I1 | 3.55 (err=0.14) | 3.69 (err=0.00) | **MLE** |
| U1 | I2 | 3.25 (err=0.49) | 3.74 (err=0.00) | **MLE** |
| U2 | I1 | 3.20 (err=0.50) | 3.66 (err=0.03) | **MLE** |
| U2 | I2 | 3.45 (err=0.29) | 3.71 (err=0.03) | **MLE** |
| U3 | I1 | 3.25 (err=0.45) | 3.70 (err=0.00) | **MLE** |
| U3 | I2 | 3.30 (err=0.44) | 3.75 (err=0.02) | **MLE** |

### Summary Statistics

| Metric | Mean-Fill (Point 11) | MLE (Point 6) |
|--------|----------------------|---------------|
| Total Error | 2.31 | 0.08 |
| Avg Error | 0.385 | **0.013** |
| Min Error | 0.14 | 0.00 |
| Max Error | 0.50 | 0.03 |
| Improvement | - | **97% better** |

### Comments

1. **MLE wins in ALL 6 cases** - 97% lower average error (0.013 vs 0.385)

2. **Top-10 improves both methods but MLE still dominates:**
   - Mean-Fill improved from 0.507 (Top-5) to 0.385 (Top-10) = 24% better
   - MLE improved from 0.033 (Top-5) to 0.013 (Top-10) = 60% better

3. **Mean-Fill still shows downward bias** - All predictions (3.20-3.55) are below item mean, even with 10 dimensions

4. **MLE achieves near-perfect predictions** - 3 out of 6 predictions have 0.00 error

5. **Gap between methods is even larger with Top-10:**
   - Top-5 comparison: MLE 93% better
   - Top-10 comparison: MLE 97% better

**Conclusion:** Increasing dimensions benefits both methods, but MLE's reconstruction approach maintains its significant advantage. MLE is the clear winner for both Top-5 and Top-10 configurations.

---

## Conclusion

The PCA MLE method successfully reduces dimensionality while maintaining prediction accuracy. **Top-10 PCs is recommended** as it achieves significantly lower error than Top-5 PCs while still providing meaningful dimensionality reduction.

The reconstruction-based prediction formula works well, producing predictions very close to item means for users without actual ratings on target items.

