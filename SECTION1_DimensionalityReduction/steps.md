# Part 2: PCA Method with Maximum Likelihood Estimation (MLE)

## Phase 1: Covariance Matrix Generation (Step 1)
**Context:** We calculate the relationships between items using only "real" data. This avoids the bias of inserting fake average values.

**1. Calculate Item Means (Preprocessing)**
* **Goal:** Compute the mean for every item $j$ using only observed ratings.
* **Formula:**
    $$\mu_j = \frac{\sum_{u \in Observed(j)} R_{u,j}}{|Observed(j)|}$$

**2. Center the Data**
* **Goal:** Create centered deviations.
* **Logic:**
    * If user $u$ rated item $j$: $X'_{u,j} = R_{u,j} - \mu_j$
    * If rating is missing: Ignore it (do not use 0 or mean).

**3. Generate Covariance Matrix ($\Sigma$)**
* **Goal:** Compute the $N \times N$ covariance matrix (where $N$ is the number of items).
* [cite_start]**Rule (from Image & Lecture):** "Estimate covariance between only the specified entries... if no users in common, covariance is 0" [cite: 327, 441-442].
* **Formula:** For every pair of items $(i, j)$:
    $$Cov(i, j) = \frac{\sum_{u \in Common(i,j)} (X'_{u,i} \times X'_{u,j})}{|Common(i,j)| - 1}$$
    * *Where:* $Common(i,j)$ is the set of users who rated **both** items.
    * *Constraint:* If $|Common(i,j)| < 2$, set $Cov(i,j) = 0$.

---

## Phase 2: Eigen Decomposition (Step 2)
**Context:** Identify the "Peers" (Principal Components) that capture the true underlying structure of the data.

**4. Determine Top k-Peers**
* **Action:** Perform Eigen Decomposition on $\Sigma_{MLE}$.
* **Formula:** Solve $\det(\Sigma_{MLE} - \lambda I) = 0$.
* **Selection:**
    * **Top 5-Peers ($W_5$):** The 5 eigenvectors corresponding to the largest eigenvalues.
    * **Top 10-Peers ($W_{10}$):** The 10 eigenvectors corresponding to the largest eigenvalues.

---

## Phase 3: Dimensionality Reduction (Steps 3 & 5)
**Context:** Project users into the "Latent Space." Even though users have missing data, we can find their position in the reduced space using the items they *did* rate.

**5. Calculate User Scores (Latent Variables)**
* **Goal:** Find the coordinate vector $T_u$ for each user $u$.
* **Formula:**
    $$T_u = X'_u \times W_k$$
    * *Calculation:* For a user $u$ and component $p$:
        $$t_{u,p} = \sum_{j \in Observed(u)} (R_{u,j} - \mu_j) \times W_{j,p}$$
    * *Note:* As shown in the lecture (Slide 20), we only sum over the items the user explicitly rated[cite: 555].

---

## Phase 4: Prediction via Reconstruction (Steps 4 & 6)
**Context:** In the MLE method, prediction is typically done by "reconstructing" the rating from the latent factors, rather than finding neighbors [cite: 567-569].

**6. Compute Rating Prediction**
* **Goal:** Predict rating for User $u$ on Target Item $i$ (I1 or I2).
* **Formula (Reconstruction):**
    $$\hat{r}_{u,i} = \mu_i + \sum_{p=1}^k (t_{u,p} \times W_{i,p})$$
    * *Where:*
        * $\mu_i$: The mean of the target item.
        * $t_{u,p}$: The user's score on component $p$ (calculated in Step 5).
        * $W_{i,p}$: The weight (loading) of item $i$ on component $p$.
        * $k$: The number of peers used (5 or 10).

---

## Phase 5: Comparisons (Steps 7, 8, 9)

**7. Compare Point 3 (Top 5) vs Point 6 (Top 10)**
* **Context:** Comparing dimensionality within the MLE method.
* **Analysis:**
    * **Top 5:** Projects onto the 5 strongest trends. Likely cleaner but simpler.
    * **Top 10:** Projects onto 10 trends. Captures more variance but may include noise (small eigenvalues).
    * *Check:* Do the predictions change drastically? If they are stable, the first 5 components capture most of the signal.

**8. Compare Part 1 (Mean-Filling Top 5) vs Part 2 (MLE Top 5)**
* **Context:** Comparing the two different PCA methodologies.
* **Analysis:**
    * **Mean-Filling:** Tends to **underestimate** variance because filling with means "flattens" the data. Predictions might be closer to the average (e.g., closer to 4)[cite: 505, 510].
    * **MLE:** Preserves the full magnitude of relationships. [cite_start]Predictions are often more distinct (e.g., predicting a 2 or a 6 instead of a 4) [cite: 601-602].

**9. Compare Part 1 (Mean-Filling Top 10) vs Part 2 (MLE Top 10)**
* **Analysis:** Similar to above. MLE should show stronger correlations and potentially more accurate predictions for sparse items, whereas Mean-Filling might suffer from "noise" generated by the fake mean values.